{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0918c93e",
   "metadata": {},
   "source": [
    "# Classifying Skin Cancer Microscopy Images Using EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe623da",
   "metadata": {},
   "source": [
    "This project uses the HAM10000 dataset of microscopic images of skin cancer cells to identify the different subclasses of skin carcinoma using neural netowrks and optimized segmentation and classification with EfficientNet. \n",
    "\n",
    "Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions:\n",
    "\n",
    "1. Actinic keratoses and intraepithelial carcinoma / Bowen's disease (AKIEC),\n",
    "2. Basal cell carcinoma (BCC),\n",
    "3. Benign keratosis-like lesions (solar lentigines / seborrheic keratoses and lichen-planus like keratoses, BKL),\n",
    "3. Dermatofibroma (DF),\n",
    "4. Melanoma (MEL),\n",
    "5. Melanocytic nevi (NV)\n",
    "6. Vascular lesions (angiomas, angiokeratomas, pyogenic granulomas and hemorrhage, VASC).\n",
    "\n",
    "#### Acknowledgements\n",
    "Tschandl, P., Rinner, C., Apalla, Z. et al. Humanâ€“computer collaboration for skin cancer recognition. Nat Med (2020). https://doi.org/10.1038/s41591-020-0942-0\n",
    "\n",
    "Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci Data 5, 180161 (2018). https://doi.org/10.1038/sdata.2018.161\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adedea3",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de751439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import cv2 as cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.core.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424d98b",
   "metadata": {},
   "source": [
    "### Import images and extract the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36371d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath=r'/Users/erfanshekarriz/git_repo/ML_miniprojects/skincanc_neuralnetworks/data/images/ISIC_0024314.jpg'\n",
    "# plot the image\n",
    "img=plt.imread(fpath)\n",
    "print (img.shape)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83588f31",
   "metadata": {},
   "source": [
    "### Read the classification file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c17b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'/Users/erfanshekarriz/git_repo/ML_miniprojects/skincanc_neuralnetworks/data/GroundTruth.csv')\n",
    "print (df.head())\n",
    "print (len(df))\n",
    "print (df.columns)\n",
    "\n",
    "# Add the '.jpg' label to match with image names\n",
    "df['image']=df['image'].apply(lambda x: x+ '.jpg')\n",
    "print (df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621b849",
   "metadata": {},
   "source": [
    "### Convert data to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\n",
    "label_list=[]\n",
    "for i in range (len(df)):\n",
    "    row= list(df.iloc[i])\n",
    "    del row[0]\n",
    "    index=np.argmax(row)\n",
    "    label=labels[index]\n",
    "    label_list.append(label)\n",
    "df['label']= label_list\n",
    "df=df.drop(labels, axis=1)\n",
    "print (df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992d476",
   "metadata": {},
   "source": [
    "### Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=.95 # set this to the percentof the data you want to use for training\n",
    "valid_split=.025 # set this to the percent of the data you want to use for validation\n",
    "# Note percent of data sed for test is 1-train_split-valid_split\n",
    "dummy_split=valid_split/(1-train_split)\n",
    "train_df, dummy_df=train_test_split(df, train_size=train_split, shuffle=True, random_state=123)\n",
    "valid_df, test_df=train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=123)\n",
    "print(' train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \n",
    "print (train_df.head())\n",
    "print (train_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fea06a",
   "metadata": {},
   "source": [
    "### Balance categories by sampling to depth of 300 representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('original number of classes: ', len(df['label'].unique()))     \n",
    "size=300 # set number of samples for each class\n",
    "samples=[]\n",
    "group=df.groupby('label')\n",
    "for label in df['label'].unique():\n",
    "    Lgroup=group.get_group(label)\n",
    "    count=int(Lgroup['label'].value_counts())    \n",
    "    if count>=size:\n",
    "        sample=Lgroup.sample(size, axis=0)        \n",
    "    else:        \n",
    "        sample=Lgroup.sample(frac=1, axis=0)\n",
    "    samples.append(sample) \n",
    "train_df=pd.concat(samples, axis=0).reset_index(drop=True)\n",
    "print (len(train_df))\n",
    "print ('final number of classes: ', len(train_df['label'].unique()))       \n",
    "print (train_df['label'].value_counts())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8386aa7",
   "metadata": {},
   "source": [
    "### Set image, input, and output parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66230c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main directory where data is stored\n",
    "sdir=r'/Users/erfanshekarriz/git_repo/ML_miniprojects/skincanc_neuralnetworks/data/images' \n",
    "\n",
    "# output directory where model will be saved\n",
    "save_dir=r'./' \n",
    "\n",
    "# part of the name of the saved model\n",
    "subject='cancer' \n",
    "\n",
    "# image height\n",
    "height=224\n",
    "\n",
    "# image width\n",
    "width=224   \n",
    "\n",
    "# number of color channels\n",
    "channels=3\n",
    "\n",
    "# model batch size for training and evaluation\n",
    "batch_size=40  \n",
    "\n",
    "img_shape=(height, width, channels)\n",
    "img_size=(height, width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa47ba",
   "metadata": {},
   "source": [
    "### Create generators for training, test, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the length of the batch size & number of steps \n",
    "# test_batch_size X test_steps = number of test samples\n",
    "# Ensures that predictions go through the test once only \n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) \n",
    "                        for n \n",
    "                        in range(1,length+1) \n",
    "                        if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "\n",
    "test_steps=int(length/test_batch_size)\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
    "\n",
    "\n",
    "# Scale images for non-EfficientNet model\n",
    "def scalar(img): \n",
    "    # scale pixel between -1 and +1\n",
    "    return img/127.5-1  \n",
    "gen=ImageDataGenerator() \n",
    "\n",
    "\n",
    "# Make generators \n",
    "train_gen=gen.flow_from_dataframe( train_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "test_gen=gen.flow_from_dataframe( test_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "valid_gen=gen.flow_from_dataframe( valid_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Count the classes to check the balance\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_count=len(classes)\n",
    "train_steps=int(len(train_gen.labels)/batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d18f3",
   "metadata": {},
   "source": [
    "### Further balance classes with \"class_weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives more weight to the underrepresented classes \"DF\" and \"VASC\"\n",
    "\n",
    "class_weight={}\n",
    "train_dict=train_gen.class_indices\n",
    "classes=list(train_dict.keys())\n",
    "class_count=len(classes)\n",
    "labels=train_gen.labels\n",
    "count_array=np.zeros((class_count))\n",
    "\n",
    "# Integer values for labels\n",
    "for value in train_dict.values(): \n",
    "    # iterate through the train_gen labels \n",
    "    for label in labels:   \n",
    "        if label==value:\n",
    "            count_array[value] +=1  \n",
    "            \n",
    "            \n",
    "max_samples=np.max(count_array)\n",
    "max_index=np.argmax(count_array)\n",
    "max_class=classes[max_index]\n",
    "\n",
    "print('class ', max_class,' with ', max_samples, 'samples has the largest sample size')\n",
    "msg='{0:^30s}{1:^10s}{2:^9s}'.format('Class', 'Samples', 'Weight')\n",
    "print(msg)\n",
    "\n",
    "\n",
    "for i in range (class_count):\n",
    "    class_weight[i]= max_samples/count_array[i]\n",
    "    msg=f'{classes[i]:^30s}{str(count_array[i]):^10s}{class_weight[i]:^9.5f}'\n",
    "    print (msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234b76e",
   "metadata": {},
   "source": [
    "### Function to show image examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23af14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_samples(gen ):\n",
    "    test_dict=test_gen.class_indices\n",
    "    classes=list(test_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i]/255 # scale images between 0 and 1 becaue no preprocessing scaling was done for efficientnet\n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=16)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d303a",
   "metadata": {},
   "source": [
    "### Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_samples(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b51f7e",
   "metadata": {},
   "source": [
    "# Creating the EfficientNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='EfficientNetB5'\n",
    "base_model=tf.keras.applications.EfficientNetB1(include_top=False, \n",
    "                                                weights=\"imagenet\",\n",
    "                                                input_shape=img_shape, \n",
    "                                                pooling='max') \n",
    "x=base_model.output\n",
    "x=keras.layers.BatchNormalization(axis=-1, \n",
    "                                  momentum=0.99, \n",
    "                                  epsilon=0.001 )(x)\n",
    "\n",
    "x = Dense(256, \n",
    "          kernel_regularizer = regularizers.l2(l = 0.016),\n",
    "          activity_regularizer=regularizers.l1(0.006),\n",
    "          bias_regularizer=regularizers.l1(0.006), \n",
    "          activation='relu')(x)\n",
    "\n",
    "x=Dropout(rate=.45, seed=123)(x)   \n",
    "\n",
    "output=Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "# create the final model\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3453be",
   "metadata": {},
   "source": [
    "### Create callback subclasses to control learning rates and print the training results for each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRA(keras.callbacks.Callback):\n",
    "    \n",
    "    reset=False\n",
    "    count=0\n",
    "    stop_count=0\n",
    "    tepochs=0\n",
    "    \n",
    "    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,batches, initial_epoch):\n",
    "        super(LRA, self).__init__()\n",
    "        self.model=model\n",
    "        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience=stop_patience\n",
    "        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor=factor # factor by which to reduce the learning rate\n",
    "        self.dwell=dwell\n",
    "        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n",
    "        self.highest_tracc=0.0 # set highest training accuracy to 0\n",
    "        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n",
    "        #self.count=0 # initialize counter that counts epochs with no improvement\n",
    "        #self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n",
    "        self.initial_epoch=initial_epoch \n",
    "        self.batches=batches\n",
    "        #self.epochs=epochs\n",
    "        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed        \n",
    "        msg=' '\n",
    "        if freeze==True:\n",
    "            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n",
    "        else:\n",
    "            msgs=f' Starting training using base model { model_name} training all layers '            \n",
    "        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n",
    "    def on_train_begin(self, logs=None):\n",
    "        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
    "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor', 'Duration', 'Batch')\n",
    "        print_in_color(msg, (244,252,3), (55,65,80)) \n",
    "        \n",
    "        \n",
    "        def on_train_batch_begin(self, batch, logs=None):\n",
    "        msg='{0:83s}{1:4s}of {2:5s}'.format(' ', str(batch), str(self.batches))\n",
    "        print(msg, '\\r', end='') # prints over on the same line to show running batch count\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self,epoch, logs=None):\n",
    "        self.now= time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        later=time.time()\n",
    "        duration=later-self.now \n",
    "        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        current_lr=lr\n",
    "        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        acc=logs.get('accuracy')  # get training accuracy \n",
    "        v_acc=logs.get('val_accuracy')\n",
    "        loss=logs.get('loss')\n",
    "        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor='accuracy'\n",
    "            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n",
    "                self.highest_tracc=acc # set new highest training accuracy\n",
    "                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n",
    "                self.count=0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count=0 # set stop counter to 0\n",
    "                if v_loss<self.lowest_vloss:\n",
    "                    self.lowest_vloss=v_loss\n",
    "                color= (0,255,0)\n",
    "                self.lr=lr\n",
    "                \n",
    "            else: \n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count>=self.patience -1:\n",
    "                    color=(245, 170, 66)\n",
    "                    self.lr= lr* self.factor # adjust the learning by factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n",
    "                    self.count=0 # reset the count to 0\n",
    "                    self.stop_count=self.stop_count + 1\n",
    "                    if self.dwell:\n",
    "                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n",
    "                    else:\n",
    "                        if v_loss<self.lowest_vloss:\n",
    "                            self.lowest_vloss=v_loss                                    \n",
    "                else:\n",
    "                    self.count=self.count +1 # increment patience counter    \n",
    "            else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor='val_loss'\n",
    "            if v_loss< self.lowest_vloss: # check if the validation loss improved \n",
    "                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n",
    "                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n",
    "                self.count=0 # reset count since validation loss improved  \n",
    "                self.stop_count=0  \n",
    "                color=(0,255,0)\n",
    "                self.lr=lr\n",
    "            else: # validation loss did not improve\n",
    "                if self.count>=self.patience-1:\n",
    "                    color=(245, 170, 66)\n",
    "                    self.lr=self.lr * self.factor # adjust the learning rate                    \n",
    "                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n",
    "                    self.count=0 # reset counter\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n",
    "                    if self.dwell:\n",
    "                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n",
    "                else: \n",
    "                    self.count =self.count +1 # increment the patience counter                    \n",
    "                if acc>self.highest_tracc:\n",
    "                    self.highest_tracc= acc\n",
    "        msg=f'{str(epoch+1):^3s}/{str(LRA.tepochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{self.lr:^9.5f}{monitor:^11s}{duration:^8.2f}'\n",
    "        print_in_color (msg,color, (55,65,80))\n",
    "        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print_in_color(msg, (0,255,0), (55,65,80))\n",
    "            self.model.stop_training = True # stop training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d65010",
   "metadata": {},
   "source": [
    "### Instantiate the callback and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =40\n",
    "patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
    "factor=.5 # factor to reduce lr by\n",
    "dwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\n",
    "freeze=False # if true free weights of  the base model\n",
    "batches=train_steps\n",
    "callbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n",
    "                   factor=factor,dwell=dwell, model_name=model_name, freeze=freeze, batches=batches,initial_epoch=0 )]\n",
    "LRA.tepochs=epochs  # used to determine value of last epoch for printing\n",
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0, class_weight=class_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cea34a",
   "metadata": {},
   "source": [
    "# Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8c421",
   "metadata": {},
   "source": [
    "### Training Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout\n",
    "    #plt.style.use('fivethirtyeight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544109d5",
   "metadata": {},
   "source": [
    "### Confusion Matrix Plot & Classify Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info( test_gen, preds, print_code, save_dir, subject ):\n",
    "    class_dict=test_gen.class_indices\n",
    "    labels= test_gen.labels\n",
    "    file_names= test_gen.filenames \n",
    "    error_list=[]\n",
    "    true_class=[]\n",
    "    pred_class=[]\n",
    "    prob_list=[]\n",
    "    new_dict={}\n",
    "    error_indices=[]\n",
    "    y_pred=[]\n",
    "    for key,value in class_dict.items():\n",
    "        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n",
    "    # store new_dict as a text fine in the save_dir\n",
    "    classes=list(new_dict.values())     # list of string of class names\n",
    "    dict_as_text=str(new_dict)\n",
    "    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n",
    "    dict_path=os.path.join(save_dir,dict_name)    \n",
    "    with open(dict_path, 'w') as x_file:\n",
    "        x_file.write(dict_as_text)    \n",
    "    errors=0      \n",
    "    for i, p in enumerate(preds):\n",
    "        pred_index=np.argmax(p)        \n",
    "        true_index=labels[i]  # labels are integer values\n",
    "        if pred_index != true_index: # a misclassification has occurred\n",
    "            error_list.append(file_names[i])\n",
    "            true_class.append(new_dict[true_index])\n",
    "            pred_class.append(new_dict[pred_index])\n",
    "            prob_list.append(p[pred_index])\n",
    "            error_indices.append(true_index)            \n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)    \n",
    "    if print_code !=0:\n",
    "        if errors>0:\n",
    "            if print_code>errors:\n",
    "                r=errors\n",
    "            else:\n",
    "                r=print_code           \n",
    "            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n",
    "            print_in_color(msg, (0,255,0),(55,65,80))\n",
    "            for i in range(r):                \n",
    "                split1=os.path.split(error_list[i])                \n",
    "                split2=os.path.split(split1[0])                \n",
    "                fname=split2[1] + '/' + split1[1]\n",
    "                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n",
    "                print_in_color(msg, (255,255,255), (55,65,60))\n",
    "                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n",
    "        else:\n",
    "            msg='With accuracy of 100 % there are no errors to print'\n",
    "            print_in_color(msg, (0,255,0),(55,65,80))\n",
    "    if errors>0:\n",
    "        plot_bar=[]\n",
    "        plot_class=[]\n",
    "        for  key, value in new_dict.items():        \n",
    "            count=error_indices.count(key) \n",
    "            if count!=0:\n",
    "                plot_bar.append(count) # list containg how many times a class c had an error\n",
    "                plot_class.append(value)   # stores the class \n",
    "        fig=plt.figure()\n",
    "        fig.set_figheight(len(plot_class)/3)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        for i in range(0, len(plot_class)):\n",
    "            c=plot_class[i]\n",
    "            x=plot_bar[i]\n",
    "            plt.barh(c, x, )\n",
    "            plt.title( ' Errors by Class on Test Set')\n",
    "    y_true= np.array(labels)        \n",
    "    y_pred=np.array(y_pred)\n",
    "    if len(classes)<= 30:\n",
    "        # create a confusion matrix \n",
    "        cm = confusion_matrix(y_true, y_pred )        \n",
    "        length=len(classes)\n",
    "        if length<8:\n",
    "            fig_width=8\n",
    "            fig_height=8\n",
    "        else:\n",
    "            fig_width= int(length * .5)\n",
    "            fig_height= int(length * .5)\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
    "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes)\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47458e3",
   "metadata": {},
   "source": [
    "# Evlauate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155574d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
